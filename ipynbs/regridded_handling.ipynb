{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Regrid Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to have a streamlined way to access the files a user needs given a set of parameters. Envisioning something where a user could enter:\n",
    "* Sectors of interest (or \"all\")\n",
    "* Timerange of interest\n",
    "* Spatial bounding box\n",
    "* Gas species of interest    \n",
    "\n",
    "And be returned either an xarray object with dask parallelization, or create a new netCDF/set of netCDF files to be loaded by the user later. \n",
    "\n",
    "There is obviously lots of work to be done, but this is a good start if you want to start playing with data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import xesmf as xe\n",
    "import calendar\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import noaa_csl_funcs as ncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_extent={'lon_min':-112.4,\n",
    "            'lon_max':-111.4,\n",
    "            'lat_min':40.1,\n",
    "            'lat_max':41.3} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrid_id = '_0.025deg'\n",
    "regridded_path = f'/uufs/chpc.utah.edu/common/home/lin-group9/agm/NOAA_CSL_Data/regridded{regrid_id}'\n",
    "RCH = ncf.Regridded_CSL_Handler(regridded_path)\n",
    "dt1  = pd.to_datetime('2019-01-01 00') \n",
    "dt2 = pd.to_datetime('2019-02-28 23') \n",
    "day_types = ['weekdy','satdy','sundy'] #a list with any or all of 'weekdy','satdy','sundy'\n",
    "species = ['CO2','CO','HC01','HC02','HC14','NH3','NOX','SO2']\n",
    "dataset_extent = {'lon_min':-112.1,\n",
    "                  'lon_max':-111.6,\n",
    "                  'lat_min':40.3,\n",
    "                  'lat_max':41.1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_types = ['area','point']\n",
    "combined_dss = {}\n",
    "for sector_type in sector_types:\n",
    "    #Get the paths to the files that match the criteria\n",
    "    days_paths = RCH.get_days_in_range(dt1,dt2,day_types,sector_type) \n",
    "    files = RCH.get_files_in_days(days_paths)\n",
    "\n",
    "    load_vars = [] #the variables we want to load\n",
    "    if sector_type == 'area': #for area sources, just load the species defined above\n",
    "        load_vars = species.copy() \n",
    "    if sector_type == 'point': #for point, often useful to have the stack/type information \n",
    "        load_vars = species.copy()\n",
    "        load_vars.extend(['ITYPE','STKht','STKdiam','STKtemp','STKve','STKflw','FUGht']) #so add it to the actual species\n",
    "\n",
    "    #Load the files with xarray, preprocessing them so they can be combined by coordinates\n",
    "    ds_list = [] #initialize the list of datasets\n",
    "    for file in files:\n",
    "        ds = RCH.preprocess_regridded(xr.open_dataset(file,chunks = {'utc_hour':1}),dataset_extent)[load_vars] #prepreprocess the file, open with dask chunking, and only keep the species of interest\n",
    "        ds_list.append(ds)  \n",
    "    ds_combined = xr.combine_by_coords(ds_list,combine_attrs='drop_conflicts') #this is the combined dataset!\n",
    "    combined_dss[sector_type] = ds_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rework_datetime(ds):\n",
    "    combined_ds_list = []\n",
    "    for yr_mo in ds.yr_mo.values:\n",
    "        year, month = map(int,yr_mo.split('-'))\n",
    "        month_ds = ds.sel(yr_mo=yr_mo)\n",
    "\n",
    "        dates_in_month = pd.date_range(start=f'{year}-{month:02}-01', end=f'{year}-{month:02}-{calendar.monthrange(year, month)[1]}')\n",
    "        dates_by_day_type = {\n",
    "            'weekdy':[date for date in dates_in_month if date.weekday() < 5],\n",
    "            'satdy':[date for date in dates_in_month if date.weekday() == 5],\n",
    "            'sundy':[date for date in dates_in_month if date.weekday() == 6]\n",
    "        }\n",
    "\n",
    "        new_month_ds_list = []\n",
    "        for day_type,dates in dates_by_day_type.items():\n",
    "            subds = month_ds.sel(day_type=day_type).drop_vars(['day_type','yr_mo'])\n",
    "            subds = subds.assign_coords({'date':dates})\n",
    "            datetimes = [pd.Timestamp(date) + pd.Timedelta(hours=int(hour)) for date in subds.coords['date'].values for hour in subds.coords['utc_hour'].values]\n",
    "            datetime_index = pd.DatetimeIndex(datetimes)\n",
    "\n",
    "            subds = subds.stack({'datetime':('date','utc_hour')})#.assign_coords({'datetime':datetime_index})\n",
    "            subds = subds.drop_vars(['date','utc_hour','datetime'])\n",
    "            subds = subds.assign_coords({'datetime':datetime_index})\n",
    "            new_month_ds_list.append(subds)\n",
    "\n",
    "        new_month_ds = xr.concat(new_month_ds_list,dim='datetime').sortby('datetime')\n",
    "        combined_ds_list.append(new_month_ds)\n",
    "\n",
    "    combined_ds = xr.concat(combined_ds_list,dim='datetime').sortby('datetime')\n",
    "    return combined_ds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_dss = {}\n",
    "for sector_type in sector_types:\n",
    "    dt_dss[sector_type] = rework_datetime(combined_dss[sector_type]).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to nc\n",
    "save_path = '/uufs/chpc.utah.edu/common/home/u0890904/LAIR_1/Data/NC'\n",
    "save_prefix = 'slv_2019_dt'\n",
    "for sector_type in sector_types:\n",
    "    dt_dss[sector_type].to_netcdf(os.path.join(save_path,f\"{save_prefix}_{sector_type}.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from presaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrid_id = 2\n",
    "regridded_path = f'/uufs/chpc.utah.edu/common/home/lin-group9/agm/NOAA_CSL_Data/regridded{regrid_id}'\n",
    "RCH = ncf.Regridded_CSL_Handler(regridded_path)\n",
    "day_types = ['weekdy','satdy','sundy'] #a list with any or all of 'weekdy','satdy','sundy'\n",
    "\n",
    "save_path = '/uufs/chpc.utah.edu/common/home/u0890904/LAIR_1/Data/NC'\n",
    "save_prefix = f'slc_2019_em27_{regrid_id}'\n",
    "\n",
    "#Load from nc\n",
    "sector_types = ['area','point']\n",
    "combined_dss = {}\n",
    "for sector_type in sector_types:\n",
    "    combined_dss[sector_type] = xr.load_dataset(os.path.join(save_path,f\"{save_prefix}_{sector_type}.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.PlateCarree()\n",
    "fig,axs = plt.subplots(1,2, figsize = (24,10), subplot_kw={'projection':proj})\n",
    "\n",
    "request = cimgt.GoogleTiles(style='satellite')\n",
    "scale = 9.0\n",
    "\n",
    "for i in range(0,2):\n",
    "    axs[i].set_extent([map_extent['lon_min'],map_extent['lon_max'],map_extent['lat_min'],map_extent['lat_max']],crs=proj)\n",
    "    axs[i].add_image(request,int(scale))\n",
    "\n",
    "sector = 'area_onroad_gasoline'\n",
    "var = 'CO2'\n",
    "year = 2019\n",
    "month = 1\n",
    "day = 1\n",
    "day_type = 'weekdy'\n",
    "hour = 11\n",
    "\n",
    "\n",
    "orig_plot_ds = area_ds.sel(sector=sector,day_type = day_type, yr_mo = f'{year}-{month}',utc_hour = hour)[var]\n",
    "orig_plot = orig_plot_ds.plot.pcolormesh('lon','lat',ax=axs[0],alpha = 0.5,transform=proj,add_colorbar=True)\n",
    "\n",
    "combined_plot_ds = combined_ds.sel(sector=sector,datetime=pd.Timestamp(f'{year}-{month}-{day} 00:00:00')+pd.Timedelta(hours=hour))[var]\n",
    "combined_plot = combined_plot_ds.plot.pcolormesh('lon','lat',ax=axs[1],alpha = 0.5,transform=proj,add_colorbar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Older stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ds = combined_dss['area']\n",
    "day_sum_ds = xr.combine_by_coords([\n",
    "    area_ds[spec].sum(dim='utc_hour').assign_attrs(\n",
    "        {'units': f\"{area_ds[spec].attrs['units'].split()[0]} day^-1 meters^-2\"}\n",
    "    ) \n",
    "    for spec in area_ds.data_vars\n",
    "    ])\n",
    "\n",
    "month_sums = []\n",
    "for yr_mo in day_sum_ds.yr_mo.values:\n",
    "    yr = int(yr_mo.split('-')[0]) #get the year int\n",
    "    mo = int(yr_mo.split('-')[1]) #get the mnoth int\n",
    "    sat,sun,wkdy = ncf.ncount_satsunwkd(yr,mo) #for that year and month, get the number of saturdays, sundays, and weekdays\n",
    "    sat_sum = (day_sum_ds.sel(yr_mo=yr_mo,day_type='satdy')*sat).reset_coords('day_type',drop=True)#.assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    sun_sum = (day_sum_ds.sel(yr_mo=yr_mo,day_type='sundy')*sun).reset_coords('day_type',drop=True)#.assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    wkdy_sum = (day_sum_ds.sel(yr_mo=yr_mo,day_type='weekdy')*wkdy).reset_coords('day_type',drop=True)#.assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    #wkdy_sum\n",
    "    month_sum = sat_sum + sun_sum + wkdy_sum\n",
    "    for var in day_sum_ds.data_vars:\n",
    "        mass_unit = day_sum_ds[var].attrs['units'].split()[0]\n",
    "        month_sum[var].attrs.update({'units':f'{mass_unit} month^-1 m^-2'})\n",
    "    month_sums.append(month_sum)#.reset_coords('yr_mo',drop=True))\n",
    "\n",
    "yr_sum = xr.concat(month_sums,dim='yr_mo').sum(dim='yr_mo')\n",
    "for var in yr_sum.data_vars:\n",
    "    mass_unit = month_sums[0][var].attrs['units'].split()[0]\n",
    "    yr_sum[var].attrs.update({'units':f'{mass_unit} yr^-1 m^-2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_area = xr.open_dataset(f'../regridding/grid_area/grid_out_area{regrid_id}.nc')  #load the gridcell area file\n",
    "grid_area = ncf.slice_extent(grid_area,dataset_extent) #slice it to the same extent\n",
    "absolute_emissions = (yr_sum * grid_area['cell_area'])#.assign_attrs({'units':mass_unit}) #multiply by the yearsum\n",
    "\n",
    "for var in yr_sum.data_vars:\n",
    "    mass_unit = yr_sum[var].attrs['units'].split()[0]\n",
    "    absolute_emissions[var].attrs.update({'units':f'{mass_unit} yr^-1 gridcell-1'})\n",
    "\n",
    "absolute_emissions['HC01'] = (absolute_emissions['HC01']*16.04/1E6).assign_attrs({'units':'metric_Ton yr^-1 gridcell^-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = ['HC01','CO2','CO']\n",
    "\n",
    "point_ds = combined_dss['point']\n",
    "day_sum_ds = xr.combine_by_coords([\n",
    "    point_ds[spec].sum(dim='utc_hour').assign_attrs(\n",
    "        {'units': f\"{point_ds[spec].attrs['units'].split()[0]} day^-1\"}\n",
    "    ) \n",
    "    for spec in species\n",
    "    ])\n",
    "\n",
    "month_sums = []\n",
    "for yr_mo in day_sum_ds.yr_mo.values:\n",
    "    yr = int(yr_mo.split('-')[0]) #get the year int\n",
    "    mo = int(yr_mo.split('-')[1]) #get the mnoth int\n",
    "    sat,sun,wkdy = ncf.ncount_satsunwkd(yr,mo) #for that year and month, get the number of saturdays, sundays, and weekdays\n",
    "    sat_sum = (day_sum_ds.sel(yr_mo=yr_mo,day_type='satdy')*sat).reset_coords('day_type',drop=True)#.assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    sun_sum = (day_sum_ds.sel(yr_mo=yr_mo,day_type='sundy')*sun).reset_coords('day_type',drop=True)#.assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    wkdy_sum = (day_sum_ds.sel(yr_mo=yr_mo,day_type='weekdy')*wkdy).reset_coords('day_type',drop=True)#.assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    #wkdy_sum\n",
    "    month_sum = sat_sum + sun_sum + wkdy_sum\n",
    "    for var in day_sum_ds.data_vars:\n",
    "        mass_unit = day_sum_ds[var].attrs['units'].split()[0]\n",
    "        month_sum[var].attrs.update({'units':f'{mass_unit} month^-1'})\n",
    "    month_sums.append(month_sum)#.reset_coords('yr_mo',drop=True))\n",
    "\n",
    "yr_sum = xr.concat(month_sums,dim='yr_mo').sum(dim='yr_mo')\n",
    "for var in yr_sum.data_vars:\n",
    "    mass_unit = month_sums[0][var].attrs['units'].split()[0]\n",
    "    yr_sum[var].attrs.update({'units':f'{mass_unit} year^-1'})\n",
    "\n",
    "yr_sum['HC01'] = (yr_sum['HC01']*16.04/1E6).assign_attrs({'units':'metric_Ton yr^-1'})\n",
    "\n",
    "point_df = yr_sum['HC01'].to_dataframe()\n",
    "point_df = point_df.reset_index().drop(columns=['ROW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "landfills = {'slv_landfill':[40.746,-112.042],\n",
    "             'jordan_landfill':[40.55862,-112.053],\n",
    "             'davis_landfill':[41.114,-111.931],\n",
    "             'weber_landfill':[41.218,-111.99],\n",
    "             'bountiful_landfill':    [40.911,-111.917]}\n",
    "ww_plants = {'central_valley_wwtp':[40.7036613,-111.9141398],\n",
    "             'big_cottonwood_wwtp':[40.6187424,-111.7824328],\n",
    "             'se_regional_wwtp':[40.5411975,-111.8191652],\n",
    "             'south_valley_wwtp':[40.5033357,-111.9187493],\n",
    "             'slc_wwtp':[40.8030915,-111.9295899],\n",
    "             }\n",
    "refineries = {'Chevron':        [40.825,-111.924],\n",
    "              'Big West Oil':   [40.838,-111.920],\n",
    "              'Marathon':       [40.794,-111.909],\n",
    "              'Holly Refining': [40.887,-111.904],\n",
    "              'Silver Eagle':   [40.868,-111.910]}\n",
    "\n",
    "point_sources = pd.concat([pd.DataFrame(landfills),pd.DataFrame(ww_plants),pd.DataFrame(refineries)],axis=1).T\n",
    "point_sources.columns = ['lat','lon']\n",
    "point_sources['type'] = point_sources.apply(lambda x: 'landfill' if x.name in landfills.keys() else 'wwtp' if x.name in ww_plants.keys() else 'refinery',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_extent={'lon_min':-112.2,\n",
    "        'lon_max':-111.55,\n",
    "        'lat_min':40.3,\n",
    "        'lat_max':41.1} \n",
    "\n",
    "dataset_extent = {'lon_min':-112.16,\n",
    "                  'lon_max':-111.7,\n",
    "                  'lat_min':40.4,\n",
    "                  'lat_max':41.0} \n",
    "\n",
    "var = 'HC01'\n",
    "\n",
    "plotds = ncf.slice_extent(absolute_emissions[var].sum(dim = 'sector'),dataset_extent).assign_attrs({'units':'metric_Ton yr^-1 gridcell^-1'})\n",
    "print(plotds.sum())\n",
    "\n",
    "labsize = 12\n",
    "proj = ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection = proj)\n",
    "ax.set_extent([map_extent['lon_min'],map_extent['lon_max'],map_extent['lat_min'],map_extent['lat_max']],crs=proj)\n",
    "\n",
    "request = cimgt.GoogleTiles(style='street')\n",
    "scale = 10.0 # prob have to adjust this\n",
    "ax.add_image(request,int(scale))\n",
    "\n",
    "plotds.plot.pcolormesh('lon','lat',ax = ax,cmap = 'viridis',alpha = 0.7)\n",
    "\n",
    "ax.scatter(point_sources.loc[point_sources['type']=='landfill']['lon'],point_sources.loc[point_sources['type']=='landfill']['lat'],c = 'red',s = 100,transform = proj,label = 'landfill')\n",
    "ax.scatter(point_sources.loc[point_sources['type']=='wwtp']['lon'],point_sources.loc[point_sources['type']=='wwtp']['lat'],c = 'blue',s = 100,transform = proj,label = 'wwtp')\n",
    "ax.scatter(point_sources.loc[point_sources['type']=='refinery']['lon'],point_sources.loc[point_sources['type']=='refinery']['lat'],c = 'green',s = 100,transform = proj,label = 'refinery')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(f'All Sectors Summed',fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_extent={'lon_min':-112.2,\n",
    "        'lon_max':-111.55,\n",
    "        'lat_min':40.3,\n",
    "        'lat_max':41.1} \n",
    "\n",
    "dataset_extent = {'lon_min':-112.16,\n",
    "                  'lon_max':-111.7,\n",
    "                  'lat_min':40.4,\n",
    "                  'lat_max':41.0} \n",
    "\n",
    "\n",
    "for area_sector in absolute_emissions['sector']:\n",
    "        sector_tag = str(area_sector.values).split('_')[1]\n",
    "\n",
    "        plotds = ncf.slice_extent(absolute_emissions['HC01'].sel(sector=area_sector),dataset_extent)\n",
    "        point_plot_df = point_df[point_df['sector'] == f'point_{sector_tag}']\n",
    "\n",
    "        labsize = 12\n",
    "        proj = ccrs.PlateCarree()\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = plt.axes(projection = proj)\n",
    "        ax.set_extent([map_extent['lon_min'],map_extent['lon_max'],map_extent['lat_min'],map_extent['lat_max']],crs=proj)\n",
    "\n",
    "        request = cimgt.GoogleTiles(style='street')\n",
    "        scale = 10.0 # prob have to adjust this\n",
    "        ax.add_image(request,int(scale))\n",
    "\n",
    "\n",
    "\n",
    "        plotds.plot.pcolormesh('lon','lat',ax = ax,cmap = 'viridis',alpha = 0.7)\n",
    "        #ax.scatter(point_plot_df['lon'],point_plot_df['lat'],c = point_plot_df['HC01'],cmap = 'Reds',transform = proj)\n",
    "        ax.coastlines()\n",
    "        ax.add_feature(cfeature.BORDERS)\n",
    "        ax.add_feature(cfeature.STATES)\n",
    "\n",
    "        ax.scatter(point_sources.loc[point_sources['type']=='landfill']['lon'],point_sources.loc[point_sources['type']=='landfill']['lat'],c = 'red',s = 100,transform = proj,label = 'landfill')\n",
    "        ax.scatter(point_sources.loc[point_sources['type']=='wwtp']['lon'],point_sources.loc[point_sources['type']=='wwtp']['lat'],c = 'blue',s = 100,transform = proj,label = 'wwtp')\n",
    "        ax.scatter(point_sources.loc[point_sources['type']=='refinery']['lon'],point_sources.loc[point_sources['type']=='refinery']['lat'],c = 'green',s = 100,transform = proj,label = 'refinery')\n",
    "\n",
    "                \n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dss['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotds = combined_dss['area'].sel({'sector':'area_onroad_gasoline','utc_hour':0,'day_type':'weekdy','yr_mo':'2019-1'})['CO2']\n",
    "\n",
    "map_extent={'lon_min':-112.2,\n",
    "        'lon_max':-111.55,\n",
    "        'lat_min':40.3,\n",
    "        'lat_max':41.1} \n",
    "\n",
    "dataset_extent = {'lon_min':-112.16,\n",
    "                  'lon_max':-111.7,\n",
    "                  'lat_min':40.4,\n",
    "                  'lat_max':41.0} \n",
    "\n",
    "\n",
    "\n",
    "labsize = 12\n",
    "proj = ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection = proj)\n",
    "ax.set_extent([map_extent['lon_min'],map_extent['lon_max'],map_extent['lat_min'],map_extent['lat_max']],crs=proj)\n",
    "\n",
    "request = cimgt.GoogleTiles(style='street')\n",
    "scale = 10.0 # prob have to adjust this\n",
    "ax.add_image(request,int(scale))\n",
    "\n",
    "plotds.plot.pcolormesh('lon','lat',ax = ax,cmap = 'viridis',alpha = 0.7)\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(f'All Sectors Summed',fontsize = 10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noaa_csl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
