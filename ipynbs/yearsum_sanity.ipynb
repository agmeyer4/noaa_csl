{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check: Yearly SLC totals against other inventories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import xesmf as xe\n",
    "import calendar\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import noaa_csl_funcs as ncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define classes\n",
    "class Regridded_CSL_Handler:\n",
    "    '''A class to handle NOAA CSL inventory data that has been regridded and organized by regrid_data.py'''\n",
    "\n",
    "    def __init__(self,regridded_path,bau_or_covid='COVID'):\n",
    "        '''Everything revolves around the \"regridded_path\", which determines sectors via the filenames contained within'''\n",
    "\n",
    "        self.regridded_path = regridded_path\n",
    "        self.sectors = self.get_sectors()\n",
    "        self.bau_or_covid = bau_or_covid\n",
    "\n",
    "    def get_sectors(self):\n",
    "        '''Lists the sectors in the regridded data storage path'''\n",
    "\n",
    "        sector_list = ncf.listdir_visible(self.regridded_path)\n",
    "        sectors = {'area':[],'point':[]}\n",
    "        for sector in sector_list:\n",
    "            if 'area' in sector:\n",
    "                sectors['area'].append(sector)\n",
    "            elif 'point' in sector:\n",
    "                sectors['point'].append(sector)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected sector type {sector}, not point or area.\")\n",
    "        return sectors\n",
    "\n",
    "    def get_sector_subset_list(self,sector_subset):\n",
    "        '''Gets a subset of the sectors which could be one, all, or some\n",
    "        \n",
    "        Args:\n",
    "        sector_subset (str,list): \"all\" will return all sectors,  'point' will return point sectors, 'area' will return area sectors, a list will just return that list\n",
    "        \n",
    "        Returns:\n",
    "        sector_subset_list (list) : list of sectors in the subset. \n",
    "        '''\n",
    "\n",
    "        if sector_subset == 'all':\n",
    "            sector_subset_list = []\n",
    "            for k,v in self.sectors.items():\n",
    "                sector_subset_list.extend(v)\n",
    "            return sector_subset_list\n",
    "        elif type(sector_subset)==str:\n",
    "            return self.sectors[sector_subset]\n",
    "        else:\n",
    "            return sector_subset\n",
    "\n",
    "    def get_days_in_range(self,dt1,dt2,day_types,sector_subset = 'all',add_path=True):\n",
    "        '''Gets all filepaths to the day_type level that are within a datetime range\n",
    "        \n",
    "        Args:\n",
    "        dt1 (datetime.date) : a date, datetime, etc to start the range (will only use year and month)\n",
    "        dt2 (datetime.date) : a date, datetime, etc to end the range (will only use year and month)\n",
    "        sectors (list) : list of sectors to include in the list\n",
    "        day_types (list) : list of day types to include in the list\n",
    "        add_path (bool, optional) : if true (default) it will add the regridded path to each element\n",
    "\n",
    "        Returns:\n",
    "        days_in_range (list) : list of paths to files that are within the date range and sector, day_types, etc. \n",
    "        '''\n",
    "\n",
    "        dates_list = pd.date_range(dt1,dt2,freq = 'MS') #get a list of all the months between the dts\n",
    "        sector_subset_list = self.get_sector_subset_list(sector_subset)\n",
    "        days_in_range = []\n",
    "        for date in dates_list:\n",
    "            for sector in sector_subset_list:\n",
    "                for day_type in day_types:\n",
    "                    day_path = f'{sector}/{ncf.yr_to_yrstr(sector,date.year,self.bau_or_covid)}/{ncf.month_int_to_str(date.month)}/{day_type}'\n",
    "                    if add_path:\n",
    "                        days_in_range.append(os.path.join(self.regridded_path,day_path))\n",
    "                    else:\n",
    "                        days_in_range.append(day_path)\n",
    "        return days_in_range\n",
    "    \n",
    "    def get_files_in_days(self,days_paths):\n",
    "        '''Gets the files that exist in the paths\n",
    "        \n",
    "        Args:\n",
    "        days_path (list) : list of paths to the days folders\n",
    "        \n",
    "        Returns:\n",
    "        files (list) : list of files in those days' paths'''\n",
    "\n",
    "        files = []\n",
    "        for day_path in days_paths:\n",
    "            files.extend(ncf.listdir_visible(day_path,add_path=True))\n",
    "        return files\n",
    "\n",
    "\n",
    "#Define Functions\n",
    "def preprocess_regridded(ds,extent=None):\n",
    "    '''Preprocesses the regridded dataset when loaded to add attributes needed for concatenation\n",
    "    \n",
    "    Args:\n",
    "    ds (xr.DataSet) : the dataset to process\n",
    "    extent (dict) : a dictionary with 4 elements defining the bounding box -- must include 'lat_min', 'lat_max', 'lon_min', 'lon_max'. These are inclusive. Defaults to \"None\" which will return the whole ds\n",
    "    \n",
    "    Returns \n",
    "    ds (xr.DataSet) : the dataset, with added coordinates taken from the attributes, sliced to the input extent\n",
    "    '''\n",
    "\n",
    "    grid_type = ds.attrs['grid_type']\n",
    "    if grid_type == 'area':\n",
    "        ds = ds.assign_coords(sector = 'area_'+ ds.attrs['sector_id']) #add back the area, was cut off in attributes for some reason\n",
    "    elif grid_type == 'point':\n",
    "        ds = ds.assign_coords(sector = 'point_'+ ds.attrs['sector_id']) #add back the point, was cut off in attributes for some reason\n",
    "\n",
    "    ds = ds.assign_coords(day_type = ds.attrs['day_type']) #assign the day_type coordinate\n",
    "    ds = ds.assign_coords(yr_mo=f'{ds.attrs['year']}-{ds.attrs['month']}') #assign the year/month coordinate\n",
    "    ds = ds.expand_dims(dim=['sector','day_type','yr_mo']) #make the coordinates into dimensions\n",
    "    if extent is not None:\n",
    "        ds = slice_extent(ds,extent) #slice to the bounding box extent\n",
    "\n",
    "    try:\n",
    "        del ds.attrs['nc_fpath'] #we don't need this attribute -- it points to an old nc path. \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return ds\n",
    "\n",
    "def slice_extent(ds,extent):\n",
    "    '''Slice a dataset to a bounding box defined by the extent argument\n",
    "    \n",
    "    Args:\n",
    "    ds (xr.DataSet) : the dataset to process\n",
    "    extent (dict) : a dictionary with 4 elements defining the bounding box -- must include 'lat_min', 'lat_max', 'lon_min', 'lon_max' -- inclusive.\n",
    "\n",
    "    Returns:\n",
    "    ds (xr.DataSet) : the dataset clipped to within the extent range\n",
    "    '''\n",
    "\n",
    "    grid_type = ds.attrs['grid_type']\n",
    "    if grid_type == 'area': #if it's an area grid\n",
    "        ds = ds.sel(lat=slice(extent['lat_min'],\n",
    "                              extent['lat_max']),\n",
    "                    lon=slice(extent['lon_min'],\n",
    "                              extent['lon_max'])) #we can slice on the lat lon coordinates\n",
    "    elif grid_type == 'point': #if it's a point grid\n",
    "        ds = ds.where(((ds.lat>=extent['lat_min'])&\n",
    "                       (ds.lat<=extent['lat_max'])&\n",
    "                       (ds.lon>=extent['lon_min'])&\n",
    "                       (ds.lon<=extent['lon_max'])).compute(),drop=True) #we have to select the individual points using .where, and compute them. The drop term gets rid of NA's not in the extent\n",
    "    else: \n",
    "        raise TypeError('Did not recognize the grid type, unsure how to slice')\n",
    "    \n",
    "    return ds \n",
    "\n",
    "def ncount_satsunwkd(year,month):\n",
    "    '''Gets the number of saturdays, sundays and weekdays in a given month/year\n",
    "    \n",
    "    Args:\n",
    "    year (int) : the year\n",
    "    month (int) : the month, as an integer\n",
    "    \n",
    "    Returns:\n",
    "    sat_count (int) : number of saturdays\n",
    "    sun_count (int) : number of sundays\n",
    "    weekd_count (int) : number of weekdays\n",
    "    '''\n",
    "\n",
    "    num_days_in_month = calendar.monthrange(year,month)[1]\n",
    "    month_str = f'{month:02d}'\n",
    "\n",
    "    dow_ints = list(pd.date_range(start=f'{year}-{month_str}-01',end=f'{year}-{month_str}-{num_days_in_month}').weekday)\n",
    "    sat_count = len([ dow for dow in dow_ints if dow == 5 ])\n",
    "    sun_count = len([ dow for dow in dow_ints if dow == 6 ])\n",
    "    weekd_count = len([ dow for dow in dow_ints if dow < 5 ])\n",
    "    return sat_count,sun_count,weekd_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_extent={'lon_min':-112.25,\n",
    "            'lon_max':-111.55,\n",
    "            'lat_min':40.3,\n",
    "            'lat_max':41.1} \n",
    "dataset_extent = {'lon_min':-112.1,\n",
    "                  'lon_max':-111.7,\n",
    "                  'lat_min':40.4,\n",
    "                  'lat_max':41.0} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regridded_path = '/uufs/chpc.utah.edu/common/home/lin-group9/agm/NOAA_CSL_Data/regridded'\n",
    "RCH = Regridded_CSL_Handler(regridded_path)\n",
    "dt1  = '2019-01' #start year and month\n",
    "dt2 = '2019-12' #end year and month\n",
    "day_types = ['weekdy','satdy','sundy'] #a list with any or all of 'weekdy','satdy','sundy'\n",
    "species = 'HC01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get area sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = 'area'\n",
    "\n",
    "#Get the paths to the files that match the criteria\n",
    "days_paths = RCH.get_days_in_range(dt1,dt2,day_types,sectors) \n",
    "files = RCH.get_files_in_days(days_paths)\n",
    "\n",
    "#Load the files with xarray, preprocessing them so they can be combined by coordinates\n",
    "ds_list = [] #initialize the list of datasets\n",
    "for file in files:\n",
    "    ds = preprocess_regridded(xr.open_dataset(file,chunks = {'utc_hour':1}),dataset_extent,sectors)[species] #prepreprocess the file, open with dask chunking, and only keep the species of interest\n",
    "    ds_list.append(ds)  \n",
    "ds_combined = xr.combine_by_coords(ds_list,combine_attrs='drop_conflicts') #this is the combined dataset!\n",
    "mass_unit = ds_combined[species].attrs['units'].split()[0] #this will either be metric_Ton or moles depending on the species chosen\n",
    "ds = ds_combined.sum(dim=['utc_hour','sector'])[species].assign_attrs({'units':f'{mass_unit} day^-1 meters^-2'})\n",
    "month_sums = [] \n",
    "for yr_mo in ds.yr_mo.values:\n",
    "    yr = int(yr_mo.split('-')[0])\n",
    "    mo = int(yr_mo.split('-')[1])\n",
    "    sat,sun,wkdy = ncount_satsunwkd(yr,mo)\n",
    "    sat_sum = (ds.sel(yr_mo=yr_mo,day_type='satdy')*sat).assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    sun_sum = (ds.sel(yr_mo=yr_mo,day_type='sundy')*sun).assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    wkdy_sum = (ds.sel(yr_mo=yr_mo,day_type='weekdy')*wkdy).assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    month_sum = xr.combine_by_coords([sat_sum.to_dataset(name='sat'),\n",
    "                                    sun_sum.to_dataset(name='sun'),\n",
    "                                    wkdy_sum.to_dataset(name='wkdy')\n",
    "                                    ],compat='override').to_array().sum(\"variable\").drop_vars('day_type').assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    month_sums.append(month_sum.to_dataset(name=yr_mo))\n",
    "yr_sum = xr.combine_by_coords(month_sums,compat='override').to_array().sum(\"variable\").drop_vars('yr_mo').assign_attrs({'units':f'{mass_unit} meters^-2'})\n",
    "grid_area = xr.open_dataset('../regridding/grid_area/grid_out_area.nc')\n",
    "grid_area = slice_extent(grid_area,dataset_extent)\n",
    "absolute_emissions = (yr_sum * grid_area['cell_area']).assign_attrs({'units':mass_unit})\n",
    "area_sum = absolute_emissions.sum().values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get point sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = 'point'\n",
    "\n",
    "#Get the paths to the files that match the criteria\n",
    "days_paths = RCH.get_days_in_range(dt1,dt2,day_types,sectors) \n",
    "files = RCH.get_files_in_days(days_paths)\n",
    "\n",
    "#Load the files with xarray, preprocessing them so they can be combined by coordinates\n",
    "ds_list = [] #initialize the list of datasets\n",
    "for file in files:\n",
    "    ds = preprocess_regridded(xr.open_dataset(file,chunks = {'utc_hour':1}),dataset_extent,sectors)[species] #prepreprocess the file, open with dask chunking, and only keep the species of interest\n",
    "    ds_list.append(ds)  \n",
    "ds_combined = xr.combine_by_coords(ds_list,combine_attrs='drop_conflicts') #this is the combined dataset!\n",
    "mass_unit = ds_combined[species].attrs['units'].split()[0] #this will either be metric_Ton or moles depending on the species chosen\n",
    "ds = ds_combined.sum(dim=['utc_hour','sector'])[species].assign_attrs({'units':f'{mass_unit} day^-1 meters^-2'})\n",
    "month_sums = [] \n",
    "for yr_mo in ds.yr_mo.values:\n",
    "    yr = int(yr_mo.split('-')[0])\n",
    "    mo = int(yr_mo.split('-')[1])\n",
    "    sat,sun,wkdy = ncount_satsunwkd(yr,mo)\n",
    "    sat_sum = (ds.sel(yr_mo=yr_mo,day_type='satdy')*sat).assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    sun_sum = (ds.sel(yr_mo=yr_mo,day_type='sundy')*sun).assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    wkdy_sum = (ds.sel(yr_mo=yr_mo,day_type='weekdy')*wkdy).assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    month_sum = xr.combine_by_coords([sat_sum.to_dataset(name='sat'),\n",
    "                                    sun_sum.to_dataset(name='sun'),\n",
    "                                    wkdy_sum.to_dataset(name='wkdy')\n",
    "                                    ],compat='override').to_array().sum(\"variable\").drop_vars('day_type').assign_attrs({'units':f'{mass_unit} month^-1 meters^-2'})\n",
    "    month_sums.append(month_sum.to_dataset(name=yr_mo))\n",
    "yr_sum = xr.combine_by_coords(month_sums,compat='override').to_array().sum(\"variable\").drop_vars('yr_mo').assign_attrs({'units':f'{mass_unit} meters^-2'})\n",
    "point_sum = yr_sum.sum().values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1266831608.2129016 mole HC01\n"
     ]
    }
   ],
   "source": [
    "total_sum = area_sum + point_sum\n",
    "print(total_sum,mass_unit,species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20319.97899573494 metric_Ton HC01\n"
     ]
    }
   ],
   "source": [
    "if mass_unit == 'mole':\n",
    "    print(total_sum*16.04/1E6,'metric_Ton',species)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noaa_csl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
